from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import Select, WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import re
import pymysql

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ChromeDriver ---
chrome_options = Options()
# chrome_options.add_argument("--headless")  # uncomment ‡∏´‡∏≤‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ GUI
service = Service(ChromeDriverManager().install())
driver = webdriver.Chrome(service=service, options=chrome_options)

# --- URL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏ô‡πâ‡∏≤ Search ---
url_check = "https://search.tci-thailand.org/advance_search.html"

try:
    driver.get(url_check)
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, "keyword[]")))
    print("‚úÖ Successfully loaded the website:", driver.current_url)
except Exception as e:
    print("‚ùå Failed to load the website:", e)
    driver.quit()
    exit()

# --- ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• MySQL ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå ---
DB_HOST = 'localhost'
DB_USER = 'root'
DB_PASSWORD = ''  # ‡πÉ‡∏™‡πà‡∏£‡∏´‡∏±‡∏™‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
DB_NAME = 'myapp'

try:
    db_connection = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, 
                                    database=DB_NAME, charset='utf8mb4')
    cursor = db_connection.cursor()
    print("‚úÖ Connected to MySQL database:", DB_NAME)
except Exception as e:
    print("‚ùå Failed to connect to MySQL:", e)
    driver.quit()
    exit()

# ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
retrieve = "SELECT fname_en, lname_en FROM users"
cursor.execute(retrieve)
teachers = cursor.fetchall()
cursor.close()
db_connection.close()
print(f"üìå Retrieved {len(teachers)} authors from database.")

wait = WebDriverWait(driver, 10)
metadata_list = []

for teacher in teachers:
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô None
    if teacher[0] is None or teacher[1] is None:
        print("‚ö†Ô∏è Skipping teacher with missing name:", teacher)
        continue

    tname = f"{teacher[0]} {teacher[1]}"
    print(f"\nüîé Searching for author: {tname}")

    driver.get(url_check)
    try:
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dropdown ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ö‡∏ö "author"
        select_element = wait.until(EC.presence_of_element_located((By.NAME, "criteria[]")))
        select = Select(select_element)
        select.select_by_value('author')
        
        # ‡∏Å‡∏£‡∏≠‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        search_box = wait.until(EC.presence_of_element_located((By.NAME, "keyword[]")))
        search_box.clear()
        search_box.send_keys(tname)
        
        # ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏î Enter
        search_button = wait.until(EC.element_to_be_clickable((By.ID, "searchBtn")))
        search_button.click()
        
        # ‡∏´‡∏ô‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏´‡∏•‡∏î
        time.sleep(3)
        
        # ‡∏£‡∏≠‡πÉ‡∏´‡πâ element ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "Search results found"
        wait.until(lambda d: "Search results found" in d.find_element(By.ID, "search_result").text)
        print(f"‚úÖ Results loaded for {tname}")
    except Exception as e:
        print(f"‚ùå Error finding search elements for {tname}: {e}")
        continue

    try:
        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô element ‡∏ó‡∏µ‡πà‡∏°‡∏µ id "search_result"
        search_result_text = driver.find_element(By.ID, "search_result").text
        # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: "Search results found (1 item(s))"
        m = re.search(r"\((\d+)", search_result_text)
        if m:
            num_results = int(m.group(1))
        else:
            num_results = 0
        print(f"üîé Found {num_results} results for {tname}")
    except Exception as e:
        print(f"‚ùå Error retrieving search result count for {tname}: {e}")
        num_results = 0

    if num_results == 0:
        continue

    try:
        results = driver.find_elements(By.CSS_SELECTOR, "div#data-article div.card")
    except Exception as e:
        print(f"‚ùå Error retrieving search result cards for {tname}: {e}")
        continue

    for i in range(len(results)):
        try:
            # ‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ article DOM ‡∏≠‡∏≤‡∏à‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä ‡∏à‡∏∂‡∏á‡∏î‡∏∂‡∏á‡∏ú‡∏•‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            results = driver.find_elements(By.CSS_SELECTOR, "div#data-article div.card")
            result = results[i]
            article_link = result.find_element(By.CSS_SELECTOR, "p a")
            article_url = article_link.get_attribute("href")
            if not article_url.startswith("http"):
                article_url = "https://search.tci-thailand.org/" + article_url
            print(f"üîó Clicking link: {article_url}")

            current_window = driver.current_window_handle
            article_link.click()
            time.sleep(1)  # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ó‡πá‡∏ö‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏õ‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô

            all_windows = driver.window_handles
            if len(all_windows) > 1:
                new_window = [w for w in all_windows if w != current_window][0]
                driver.switch_to.window(new_window)
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "h4")))
            else:
                wait.until(EC.presence_of_element_located((By.TAG_NAME, "h4")))

            article_soup = BeautifulSoup(driver.page_source, 'lxml')

            journal = article_soup.find("span", class_="journal-name").text.strip() if article_soup.find("span", class_="journal-name") else ""
            volume = article_soup.find("span", class_="volume").text.strip() if article_soup.find("span", class_="volume") else ""
            issue = article_soup.find("span", class_="issue").text.strip() if article_soup.find("span", class_="issue") else ""
            pages = article_soup.find("span", class_="pages").text.strip() if article_soup.find("span", class_="pages") else ""
            year = article_soup.find("span", class_="year").text.strip() if article_soup.find("span", class_="year") else ""
            doi = article_soup.find("span", class_="doi").text.strip() if article_soup.find("span", class_="doi") else ""
            citation_count = article_soup.find("span", class_="citation-count").text.strip() if article_soup.find("span", class_="citation-count") else ""
            research_type = article_soup.find("span", class_="research-type").text.strip() if article_soup.find("span", class_="research-type") else ""
            
            # ‡∏î‡∏∂‡∏á Abstract (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
            abstract_elem = article_soup.find("span", id="abstract_english")
            abstract = abstract_elem.text.strip() if abstract_elem else ""
            
            # ‡∏î‡∏∂‡∏á Keywords (‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô)
            keyword_elem = article_soup.find("span", id="keyword_english")
            keywords = keyword_elem.text.strip() if keyword_elem else ""
            
            # ‡∏î‡∏∂‡∏á title ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            title_eng = article_soup.find("h4", {"id": "article_name_eng"})
            title = title_eng.text.strip() if title_eng else ""
            
            authors = [a.text.strip() for a in article_soup.find_all("span", class_="author_name")]

            metadata_list.append((title, article_url, journal, volume, issue, pages, year, doi,
                                  citation_count, research_type, abstract, keywords, ', '.join(authors)))
            print(f"‚úÖ Retrieved article: {title}")

            if len(driver.window_handles) > 1:
                driver.close()  # ‡∏õ‡∏¥‡∏î‡πÅ‡∏ó‡πá‡∏ö‡πÉ‡∏´‡∏°‡πà
                driver.switch_to.window(current_window)
            else:
                driver.back()
                wait.until(EC.presence_of_element_located((By.ID, "data-article")))
        except Exception as e:
            print(f"‚ùå Error processing article: {e}")
            try:
                if len(driver.window_handles) > 1:
                    driver.close()
                    driver.switch_to.window(current_window)
                else:
                    driver.back()
                    wait.until(EC.presence_of_element_located((By.ID, "data-article")))
            except Exception as ex:
                print(f"‚ùå Error returning to search results: {ex}")
            continue

driver.quit()
print(f"\n‚úÖ Web Scraping Completed. Retrieved {len(metadata_list)} articles.")

# --- Insert scraped data into MySQL database table tci_research_papers ---
try:
    db_connection = pymysql.connect(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, 
                                    database=DB_NAME, charset='utf8mb4')
    cursor = db_connection.cursor()
    
    insert_sql = """
    INSERT INTO tci_research_papers 
    (title, article_url, journal, volume, issue, pages, year, doi, citation_count, research_type, abstract, keywords, authors)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
    """
    
    for article in metadata_list:
        cursor.execute(insert_sql, article)
    
    db_connection.commit()
    print("‚úÖ Data inserted into tci_research_papers table successfully.")
except Exception as e:
    print("‚ùå Error inserting data into database:", e)
finally:
    cursor.close()
    db_connection.close()
